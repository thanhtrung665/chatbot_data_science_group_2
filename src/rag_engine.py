import os
import torch
from dotenv import load_dotenv
from huggingface_hub import login
from qdrant_client import QdrantClient
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# 1. Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()

class RAGPipeline:
    def __init__(self):
        """Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n cá»§a há»‡ thá»‘ng RAG"""
        print("ğŸš€ Äang khá»Ÿi táº¡o RAG Pipeline...")
        self._setup_auth()
        self._setup_qdrant()
        self._setup_models()
        print("âœ… RAG Pipeline Ä‘Ã£ sáºµn sÃ ng!")

    def _setup_auth(self):
        """ÄÄƒng nháº­p HuggingFace náº¿u cÃ³ Token"""
        hf_token = os.getenv("HF_TOKEN")
        if hf_token:
            try:
                login(token=hf_token)
                print("ğŸ”¹ ÄÃ£ login HuggingFace.")
            except Exception as e:
                print(f"âš ï¸ Login HF tháº¥t báº¡i: {e}")

    def _setup_qdrant(self):
        """Káº¿t ná»‘i Qdrant Database"""
        self.collection_name = "qa_rag_data_science"
        try:
            self.qdrant_client = QdrantClient(
                url=os.getenv("QDRANT_URL"),
                api_key=os.getenv("QDRANT_API_KEY"),
                timeout=60,
                check_compatibility=False
            )
            # Kiá»ƒm tra káº¿t ná»‘i nhanh báº±ng cÃ¡ch láº¥y info collection
            self.qdrant_client.get_collection(self.collection_name)
            print("ğŸ”¹ Káº¿t ná»‘i Qdrant thÃ nh cÃ´ng.")
        except Exception as e:
            print(f"âŒ Lá»—i káº¿t ná»‘i Qdrant: {e}")
            raise e

    def _setup_models(self):
        """Load Embedding & LLM Models"""
        # 1. Embedding Model
        print("â³ Äang load Embedding Model...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-large"
        )

        # 2. LLM Model
        print("â³ Äang load LLM (Qwen2.5-1.5b-pro)...")
        model_name = "dai3107/qwen2.5-1.5b-pro"
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()

        # Táº¡o Pipeline
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.1, # Giá»¯ tháº¥p Ä‘á»ƒ chÃ­nh xÃ¡c
            top_p=0.9,
            repetition_penalty=1.1,
            do_sample=True, # NÃªn Ä‘á»ƒ True náº¿u dÃ¹ng temperature > 0
            return_full_text=False
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)

    def retrieve_documents(self, query: str, top_k: int = 3):
        """TÃ¬m kiáº¿m vector trong Qdrant"""
        try:
            query_vector = self.embeddings.embed_query(f"query: {query}")
            search_results = self.qdrant_client.query_points(
                collection_name=self.collection_name,
                query=query_vector,
                limit=top_k,
                with_payload=True
            )
            
            # Lá»c káº¿t quáº£ cÃ³ Ä‘iá»ƒm tháº¥p (Thresholding)
            valid_results = [r for r in search_results.points if r.score > 0.35]
            
            if not valid_results:
                return None
                
            return valid_results
        except Exception as e:
            print(f"âŒ Lá»—i Retrieve: {e}")
            return None

    def generate_answer(self, query: str):
        """Quy trÃ¬nh RAG hoÃ n chá»‰nh: Retrieve -> Prompt -> Generate"""
        
        # 1. Retrieve
        print(f"ğŸ” Äang tÃ¬m kiáº¿m: {query}")
        results = self.retrieve_documents(query)
        
        if not results:
            return "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."

        # 2. Build Context
        context_texts = []
        for res in results:
            payload = res.payload
            # Æ¯u tiÃªn láº¥y field 'answer' hoáº·c 'text', fallback cÃ¡c trÆ°á»ng há»£p khÃ¡c
            content = payload.get("answer") or payload.get("tra_loi") or payload.get("text") or ""
            if content:
                context_texts.append(f"- {content}")
        
        context_str = "\n".join(context_texts)

        # 3. Build Prompt (Template chuáº©n ká»¹ sÆ°)
        prompt_template = f"""<|im_start|>system
Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn vá» Data Science. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong pháº§n NGá»® Cáº¢NH.
Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong ngá»¯ cáº£nh, hÃ£y nÃ³i "TÃ´i khÃ´ng biáº¿t". KhÃ´ng Ä‘Æ°á»£c bá»‹a Ä‘áº·t thÃ´ng tin.

NGá»® Cáº¢NH:
{context_str}
<|im_end|>
<|im_start|>user
CÃ¢u há»i: {query}
<|im_end|>
<|im_start|>assistant
"""
        # 4. Generate
        print("ğŸ¤– Äang suy nghÄ©...")
        try:
            response = self.llm.invoke(prompt_template)
            # Clean up response (Ä‘Ã´i khi pipeline tráº£ vá» cáº£ prompt)
            if hasattr(response, "content"):
                return response.content.strip()
            return str(response).strip()
        except Exception as e:
            return f"Lá»—i sinh cÃ¢u tráº£ lá»i: {str(e)}"

# =========================
# KHU Vá»°C TEST (Chá»‰ cháº¡y khi cháº¡y trá»±c tiáº¿p file nÃ y)
# =========================
if __name__ == "__main__":
    # Khá»Ÿi táº¡o Pipeline
    rag = RAGPipeline()
    
    while True:
        q = input("\nğŸ’¬ Má»i nháº­p cÃ¢u há»i (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t): ")
        if q.lower() in ["exit", "quit"]:
            break
        
        ans = rag.generate_answer(q)
        print(f"\nğŸ’¡ Tráº£ lá»i:\n{ans}\n" + "-"*50)